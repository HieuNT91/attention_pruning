{
    "attention_implementation": "flash_attention_2", 
    "max_new_tokens": 200,
    "temperature": 0.1,
    "top_k": 40,
    "top_p": 0.95,
    "repetition_penalty": 1.0,
    "num_return_sequences": 1,
    "use_cache": true,
    "do_sample": false
}